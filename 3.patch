diff --git a/src/search.rs b/src/search.rs
deleted file mode 100644
index 5c0890c888283ad6d64edfcc4c23425a9763da24..0000000000000000000000000000000000000000
--- a/src/search.rs
+++ /dev/null
@@ -1,533 +0,0 @@
-use std::cmp::{Ordering as CmpOrdering, Reverse};
-use std::collections::BinaryHeap;
-use std::sync::Arc;
-use std::sync::atomic::{AtomicU64, Ordering as AtomicOrdering};
-use std::sync::mpsc::{self, Receiver, Sender};
-use std::thread;
-
-use frizbee::{Options, match_list};
-
-#[cfg(feature = "fs")]
-use crate::indexing::{IndexUpdate, merge_update};
-use crate::types::{SearchData, SearchMode};
-
-pub(crate) const PREFILTER_ENABLE_THRESHOLD: usize = 1_000;
-pub(crate) const MAX_RENDERED_RESULTS: usize = 2_000;
-const MATCH_CHUNK_SIZE: usize = 512;
-const EMPTY_QUERY_BATCH: usize = 128;
-
-#[derive(Debug)]
-pub(crate) enum SearchCommand {
-    Query {
-        id: u64,
-        query: String,
-        mode: SearchMode,
-    },
-    #[cfg(feature = "fs")]
-    Update(IndexUpdate),
-    Shutdown,
-}
-
-#[derive(Debug)]
-pub(crate) struct SearchResult {
-    pub(crate) id: u64,
-    pub(crate) mode: SearchMode,
-    pub(crate) indices: Vec<usize>,
-    pub(crate) scores: Vec<u16>,
-    #[allow(dead_code)]
-    pub(crate) complete: bool,
-}
-
-#[cfg_attr(not(feature = "fs"), allow(unused_mut))]
-pub(crate) fn spawn(
-    mut data: SearchData,
-) -> (
-    Sender<SearchCommand>,
-    Receiver<SearchResult>,
-    Arc<AtomicU64>,
-) {
-    let (command_tx, command_rx) = mpsc::channel();
-    let (result_tx, result_rx) = mpsc::channel();
-    let latest_query_id = Arc::new(AtomicU64::new(0));
-    let thread_latest = Arc::clone(&latest_query_id);
-
-    thread::spawn(move || {
-        while let Ok(command) = command_rx.recv() {
-            match command {
-                SearchCommand::Query { id, query, mode } => {
-                    if !process_query(&data, &query, mode, id, &result_tx, &thread_latest) {
-                        break;
-                    }
-                }
-                #[cfg(feature = "fs")]
-                SearchCommand::Update(update) => {
-                    merge_update(&mut data, &update);
-                }
-                SearchCommand::Shutdown => break,
-            }
-        }
-    });
-
-    (command_tx, result_rx, latest_query_id)
-}
-
-fn process_query(
-    data: &SearchData,
-    query: &str,
-    mode: SearchMode,
-    id: u64,
-    tx: &Sender<SearchResult>,
-    latest_query_id: &AtomicU64,
-) -> bool {
-    match mode {
-        SearchMode::Facets => stream_facets(data, query, id, tx, latest_query_id),
-        SearchMode::Files => stream_files(data, query, id, tx, latest_query_id),
-    }
-}
-
-fn stream_facets(
-    data: &SearchData,
-    query: &str,
-    id: u64,
-    tx: &Sender<SearchResult>,
-    latest_query_id: &AtomicU64,
-) -> bool {
-    let trimmed = query.trim();
-    if trimmed.is_empty() {
-        return stream_alphabetical_facets(data, id, tx, latest_query_id);
-    }
-
-    let config = config_for_query(trimmed, data.facets.len());
-    let mut aggregator = ScoreAggregator::new(id, SearchMode::Facets, tx);
-    let mut offset = 0;
-    for chunk in data.facets.chunks(MATCH_CHUNK_SIZE) {
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        let haystacks: Vec<&str> = chunk.iter().map(|facet| facet.name.as_str()).collect();
-        let matches = match_list(trimmed, &haystacks, config);
-        for entry in matches {
-            if entry.score == 0 {
-                continue;
-            }
-            let index = offset + entry.index_in_haystack as usize;
-            aggregator.push(index, entry.score);
-        }
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        if !aggregator.flush_partial() {
-            return false;
-        }
-        offset += chunk.len();
-    }
-
-    if should_abort(id, latest_query_id) {
-        return true;
-    }
-
-    aggregator.finish()
-}
-
-fn stream_files(
-    data: &SearchData,
-    query: &str,
-    id: u64,
-    tx: &Sender<SearchResult>,
-    latest_query_id: &AtomicU64,
-) -> bool {
-    let trimmed = query.trim();
-    if trimmed.is_empty() {
-        return stream_alphabetical_files(data, id, tx, latest_query_id);
-    }
-
-    let config = config_for_query(trimmed, data.files.len());
-    let mut aggregator = ScoreAggregator::new(id, SearchMode::Files, tx);
-    let mut offset = 0;
-    for chunk in data.files.chunks(MATCH_CHUNK_SIZE) {
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        let haystacks: Vec<&str> = chunk.iter().map(|file| file.search_text()).collect();
-        let matches = match_list(trimmed, &haystacks, config);
-        for entry in matches {
-            if entry.score == 0 {
-                continue;
-            }
-            let index = offset + entry.index_in_haystack as usize;
-            aggregator.push(index, entry.score);
-        }
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        if !aggregator.flush_partial() {
-            return false;
-        }
-        offset += chunk.len();
-    }
-
-    if should_abort(id, latest_query_id) {
-        return true;
-    }
-
-    aggregator.finish()
-}
-
-fn stream_alphabetical_facets(
-    data: &SearchData,
-    id: u64,
-    tx: &Sender<SearchResult>,
-    latest_query_id: &AtomicU64,
-) -> bool {
-    let mut collector =
-        AlphabeticalCollector::new(id, SearchMode::Facets, tx, data.facets.len(), |index| {
-            data.facets[index].name.clone()
-        });
-
-    let mut processed = 0;
-    for (index, _) in data.facets.iter().enumerate() {
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        collector.insert(index);
-        processed += 1;
-        if processed % EMPTY_QUERY_BATCH == 0 {
-            if should_abort(id, latest_query_id) {
-                return true;
-            }
-            if !collector.flush_partial() {
-                return false;
-            }
-        }
-    }
-
-    if should_abort(id, latest_query_id) {
-        return true;
-    }
-
-    collector.finish()
-}
-
-fn stream_alphabetical_files(
-    data: &SearchData,
-    id: u64,
-    tx: &Sender<SearchResult>,
-    latest_query_id: &AtomicU64,
-) -> bool {
-    let mut collector =
-        AlphabeticalCollector::new(id, SearchMode::Files, tx, data.files.len(), |index| {
-            data.files[index].path.clone()
-        });
-
-    let mut processed = 0;
-    for (index, _) in data.files.iter().enumerate() {
-        if should_abort(id, latest_query_id) {
-            return true;
-        }
-        collector.insert(index);
-        processed += 1;
-        if processed % EMPTY_QUERY_BATCH == 0 {
-            if should_abort(id, latest_query_id) {
-                return true;
-            }
-            if !collector.flush_partial() {
-                return false;
-            }
-        }
-    }
-
-    if should_abort(id, latest_query_id) {
-        return true;
-    }
-
-    collector.finish()
-}
-
-fn should_abort(id: u64, latest_query_id: &AtomicU64) -> bool {
-    latest_query_id.load(AtomicOrdering::Acquire) != id
-}
-
-#[derive(Clone, Eq, PartialEq)]
-struct RankedMatch {
-    index: usize,
-    score: u16,
-}
-
-impl Ord for RankedMatch {
-    fn cmp(&self, other: &Self) -> CmpOrdering {
-        self.score
-            .cmp(&other.score)
-            .then_with(|| other.index.cmp(&self.index))
-    }
-}
-
-impl PartialOrd for RankedMatch {
-    fn partial_cmp(&self, other: &Self) -> Option<CmpOrdering> {
-        Some(self.cmp(other))
-    }
-}
-
-struct ScoreAggregator<'a> {
-    id: u64,
-    mode: SearchMode,
-    tx: &'a Sender<SearchResult>,
-    heap: BinaryHeap<Reverse<RankedMatch>>,
-    scratch: Vec<RankedMatch>,
-    dirty: bool,
-    sent_any: bool,
-}
-
-impl<'a> ScoreAggregator<'a> {
-    fn new(id: u64, mode: SearchMode, tx: &'a Sender<SearchResult>) -> Self {
-        Self {
-            id,
-            mode,
-            tx,
-            heap: BinaryHeap::new(),
-            scratch: Vec::new(),
-            dirty: false,
-            sent_any: false,
-        }
-    }
-
-    fn push(&mut self, index: usize, score: u16) {
-        if self.insert(RankedMatch { index, score }) {
-            self.dirty = true;
-        }
-    }
-
-    fn insert(&mut self, entry: RankedMatch) -> bool {
-        if self.heap.len() < MAX_RENDERED_RESULTS {
-            self.heap.push(Reverse(entry));
-            true
-        } else if let Some(mut current_min) = self.heap.peek_mut() {
-            if entry > current_min.0 {
-                *current_min = Reverse(entry);
-                true
-            } else {
-                false
-            }
-        } else {
-            false
-        }
-    }
-
-    fn flush_partial(&mut self) -> bool {
-        if !self.dirty {
-            return true;
-        }
-        self.emit(false)
-    }
-
-    fn finish(&mut self) -> bool {
-        if !self.emit(true) {
-            return false;
-        }
-        true
-    }
-
-    fn emit(&mut self, complete: bool) -> bool {
-        if self.heap.is_empty() && !complete && self.sent_any {
-            self.dirty = false;
-            return true;
-        }
-
-        self.scratch.clear();
-        self.scratch
-            .extend(self.heap.iter().map(|entry| entry.0.clone()));
-        self.scratch
-            .sort_unstable_by(|a, b| b.score.cmp(&a.score).then_with(|| a.index.cmp(&b.index)));
-
-        let mut indices = Vec::with_capacity(self.scratch.len());
-        let mut scores = Vec::with_capacity(self.scratch.len());
-        for entry in &self.scratch {
-            indices.push(entry.index);
-            scores.push(entry.score);
-        }
-
-        match self.tx.send(SearchResult {
-            id: self.id,
-            mode: self.mode,
-            indices,
-            scores,
-            complete,
-        }) {
-            Ok(()) => {
-                self.sent_any = true;
-                self.dirty = false;
-                true
-            }
-            Err(_) => false,
-        }
-    }
-}
-
-#[derive(Clone, Eq, PartialEq)]
-struct AlphabeticalEntry {
-    index: usize,
-    key: String,
-}
-
-impl Ord for AlphabeticalEntry {
-    fn cmp(&self, other: &Self) -> CmpOrdering {
-        self.key
-            .cmp(&other.key)
-            .then_with(|| self.index.cmp(&other.index))
-    }
-}
-
-impl PartialOrd for AlphabeticalEntry {
-    fn partial_cmp(&self, other: &Self) -> Option<CmpOrdering> {
-        Some(self.cmp(other))
-    }
-}
-
-struct AlphabeticalCollector<'a, F>
-where
-    F: Fn(usize) -> String,
-{
-    id: u64,
-    mode: SearchMode,
-    tx: &'a Sender<SearchResult>,
-    limit: usize,
-    key_for_index: F,
-    heap: BinaryHeap<AlphabeticalEntry>,
-    scratch: Vec<AlphabeticalEntry>,
-    dirty: bool,
-    sent_any: bool,
-}
-
-impl<'a, F> AlphabeticalCollector<'a, F>
-where
-    F: Fn(usize) -> String,
-{
-    fn new(
-        id: u64,
-        mode: SearchMode,
-        tx: &'a Sender<SearchResult>,
-        total: usize,
-        key_for_index: F,
-    ) -> Self {
-        Self {
-            id,
-            mode,
-            tx,
-            limit: MAX_RENDERED_RESULTS.min(total),
-            key_for_index,
-            heap: BinaryHeap::new(),
-            scratch: Vec::new(),
-            dirty: false,
-            sent_any: false,
-        }
-    }
-
-    fn insert(&mut self, index: usize) {
-        if self.limit == 0 {
-            return;
-        }
-        let entry = AlphabeticalEntry {
-            index,
-            key: (self.key_for_index)(index),
-        };
-        if self.heap.len() < self.limit {
-            self.heap.push(entry);
-            self.dirty = true;
-        } else if let Some(mut current_max) = self.heap.peek_mut()
-            && entry < *current_max
-        {
-            *current_max = entry;
-            self.dirty = true;
-        }
-    }
-
-    fn flush_partial(&mut self) -> bool {
-        if !self.dirty {
-            return true;
-        }
-        self.emit(false)
-    }
-
-    fn finish(&mut self) -> bool {
-        if self.limit == 0 {
-            return self.emit(true);
-        }
-
-        if !self.emit(true) {
-            return false;
-        }
-        true
-    }
-
-    fn emit(&mut self, complete: bool) -> bool {
-        if self.limit == 0 {
-            return self
-                .tx
-                .send(SearchResult {
-                    id: self.id,
-                    mode: self.mode,
-                    indices: Vec::new(),
-                    scores: Vec::new(),
-                    complete,
-                })
-                .is_ok();
-        }
-
-        self.scratch.clear();
-        self.scratch.extend(self.heap.iter().cloned());
-        self.scratch
-            .sort_unstable_by(|a, b| a.key.cmp(&b.key).then_with(|| a.index.cmp(&b.index)));
-
-        let mut indices = Vec::with_capacity(self.scratch.len());
-        for entry in &self.scratch {
-            indices.push(entry.index);
-        }
-        let scores = vec![0; indices.len()];
-
-        match self.tx.send(SearchResult {
-            id: self.id,
-            mode: self.mode,
-            indices,
-            scores,
-            complete,
-        }) {
-            Ok(()) => {
-                self.sent_any = true;
-                self.dirty = false;
-                true
-            }
-            Err(_) => false,
-        }
-    }
-}
-
-pub(crate) fn config_for_query(query: &str, dataset_len: usize) -> Options {
-    let mut config = Options {
-        prefilter: false,
-        ..Options::default()
-    };
-
-    let length = query.chars().count();
-    let mut allowed_typos: u16 = match length {
-        0 => 0,
-        1 => 0,
-        2..=4 => 1,
-        5..=7 => 2,
-        8..=12 => 3,
-        _ => 4,
-    };
-    if let Ok(max_reasonable) = u16::try_from(length.saturating_sub(1)) {
-        allowed_typos = allowed_typos.min(max_reasonable);
-    }
-
-    if dataset_len >= PREFILTER_ENABLE_THRESHOLD {
-        config.prefilter = true;
-        config.max_typos = Some(allowed_typos);
-    } else {
-        config.prefilter = false;
-        config.max_typos = None;
-    }
-
-    config.sort = false;
-
-    config
-}
diff --git a/src/search/aggregator.rs b/src/search/aggregator.rs
new file mode 100644
index 0000000000000000000000000000000000000000..830e296ce0905df0ee507af50048cccd61aec4ba
--- /dev/null
+++ b/src/search/aggregator.rs
@@ -0,0 +1,164 @@
+use std::cmp::{Ordering as CmpOrdering, Reverse};
+use std::collections::BinaryHeap;
+use std::sync::mpsc::Sender;
+
+use super::commands::SearchResult;
+use super::MAX_RENDERED_RESULTS;
+use crate::types::SearchMode;
+
+#[derive(Clone, Eq, PartialEq)]
+struct RankedMatch {
+    index: usize,
+    score: u16,
+}
+
+impl Ord for RankedMatch {
+    fn cmp(&self, other: &Self) -> CmpOrdering {
+        self.score
+            .cmp(&other.score)
+            .then_with(|| other.index.cmp(&self.index))
+    }
+}
+
+impl PartialOrd for RankedMatch {
+    fn partial_cmp(&self, other: &Self) -> Option<CmpOrdering> {
+        Some(self.cmp(other))
+    }
+}
+
+/// Maintains the highest scoring matches for a particular query.
+pub(super) struct ScoreAggregator<'a> {
+    id: u64,
+    mode: SearchMode,
+    tx: &'a Sender<SearchResult>,
+    heap: BinaryHeap<Reverse<RankedMatch>>,
+    scratch: Vec<RankedMatch>,
+    dirty: bool,
+    sent_any: bool,
+}
+
+impl<'a> ScoreAggregator<'a> {
+    /// Creates a new aggregator that will stream results through `tx`.
+    pub(super) fn new(id: u64, mode: SearchMode, tx: &'a Sender<SearchResult>) -> Self {
+        Self {
+            id,
+            mode,
+            tx,
+            heap: BinaryHeap::new(),
+            scratch: Vec::new(),
+            dirty: false,
+            sent_any: false,
+        }
+    }
+
+    /// Inserts a scored match and marks the aggregator as dirty when the result set changes.
+    pub(super) fn push(&mut self, index: usize, score: u16) {
+        if self.insert(RankedMatch { index, score }) {
+            self.dirty = true;
+        }
+    }
+
+    fn insert(&mut self, entry: RankedMatch) -> bool {
+        if self.heap.len() < MAX_RENDERED_RESULTS {
+            self.heap.push(Reverse(entry));
+            true
+        } else if let Some(mut current_min) = self.heap.peek_mut() {
+            if entry > current_min.0 {
+                *current_min = Reverse(entry);
+                true
+            } else {
+                false
+            }
+        } else {
+            false
+        }
+    }
+
+    /// Emits an incremental update when new matches were observed.
+    pub(super) fn flush_partial(&mut self) -> bool {
+        if !self.dirty {
+            return true;
+        }
+        self.emit(false)
+    }
+
+    /// Sends a final update for the query.
+    pub(super) fn finish(&mut self) -> bool {
+        if !self.emit(true) {
+            return false;
+        }
+        true
+    }
+
+    pub(super) fn emit(&mut self, complete: bool) -> bool {
+        if self.heap.is_empty() && !complete && self.sent_any {
+            self.dirty = false;
+            return true;
+        }
+
+        self.scratch.clear();
+        self.scratch
+            .extend(self.heap.iter().map(|entry| entry.0.clone()));
+        self.scratch
+            .sort_unstable_by(|a, b| b.score.cmp(&a.score).then_with(|| a.index.cmp(&b.index)));
+
+        let mut indices = Vec::with_capacity(self.scratch.len());
+        let mut scores = Vec::with_capacity(self.scratch.len());
+        for entry in &self.scratch {
+            indices.push(entry.index);
+            scores.push(entry.score);
+        }
+
+        match self.tx.send(SearchResult {
+            id: self.id,
+            mode: self.mode,
+            indices,
+            scores,
+            complete,
+        }) {
+            Ok(()) => {
+                self.sent_any = true;
+                self.dirty = false;
+                true
+            }
+            Err(_) => false,
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn aggregates_highest_scores() {
+        let (tx, rx) = std::sync::mpsc::channel();
+        let mut aggregator = ScoreAggregator::new(7, SearchMode::Files, &tx);
+
+        aggregator.push(0, 1);
+        aggregator.push(1, 3);
+        aggregator.push(2, 2);
+        aggregator.push(3, 3);
+
+        assert!(aggregator.finish());
+        let result = rx.try_recv().expect("result should be emitted");
+        assert_eq!(result.id, 7);
+        assert_eq!(result.indices, vec![1, 3, 2, 0]);
+        assert_eq!(result.scores, vec![3, 3, 2, 1]);
+        assert!(result.complete);
+    }
+
+    #[test]
+    fn ignores_worse_matches_when_capacity_reached() {
+        let (tx, rx) = std::sync::mpsc::channel();
+        let mut aggregator = ScoreAggregator::new(5, SearchMode::Facets, &tx);
+
+        for i in 0..super::MAX_RENDERED_RESULTS {
+            aggregator.push(i, 100);
+        }
+
+        aggregator.push(super::MAX_RENDERED_RESULTS + 1, 50);
+        assert!(aggregator.flush_partial());
+        assert!(rx.try_recv().is_ok(), "partial flush should emit");
+    }
+}
diff --git a/src/search/alphabetical.rs b/src/search/alphabetical.rs
new file mode 100644
index 0000000000000000000000000000000000000000..6d79a18e90f6d910be6c7f0a02d4e63ae6bc53f1
--- /dev/null
+++ b/src/search/alphabetical.rs
@@ -0,0 +1,182 @@
+use std::cmp::Ordering as CmpOrdering;
+use std::collections::BinaryHeap;
+use std::sync::mpsc::Sender;
+
+use super::commands::SearchResult;
+use super::MAX_RENDERED_RESULTS;
+use crate::types::SearchMode;
+
+#[derive(Clone, Eq, PartialEq)]
+struct AlphabeticalEntry {
+    index: usize,
+    key: String,
+}
+
+impl Ord for AlphabeticalEntry {
+    fn cmp(&self, other: &Self) -> CmpOrdering {
+        self.key
+            .cmp(&other.key)
+            .then_with(|| self.index.cmp(&other.index))
+    }
+}
+
+impl PartialOrd for AlphabeticalEntry {
+    fn partial_cmp(&self, other: &Self) -> Option<CmpOrdering> {
+        Some(self.cmp(other))
+    }
+}
+
+/// Collects the lexicographically smallest entries for an empty query.
+pub(super) struct AlphabeticalCollector<'a, F>
+where
+    F: Fn(usize) -> String,
+{
+    id: u64,
+    mode: SearchMode,
+    tx: &'a Sender<SearchResult>,
+    limit: usize,
+    key_for_index: F,
+    heap: BinaryHeap<AlphabeticalEntry>,
+    scratch: Vec<AlphabeticalEntry>,
+    dirty: bool,
+    sent_any: bool,
+}
+
+impl<'a, F> AlphabeticalCollector<'a, F>
+where
+    F: Fn(usize) -> String,
+{
+    /// Creates a collector that will emit at most [`MAX_RENDERED_RESULTS`] entries.
+    pub(super) fn new(
+        id: u64,
+        mode: SearchMode,
+        tx: &'a Sender<SearchResult>,
+        total: usize,
+        key_for_index: F,
+    ) -> Self {
+        Self {
+            id,
+            mode,
+            tx,
+            limit: MAX_RENDERED_RESULTS.min(total),
+            key_for_index,
+            heap: BinaryHeap::new(),
+            scratch: Vec::new(),
+            dirty: false,
+            sent_any: false,
+        }
+    }
+
+    /// Inserts a candidate index when the collector still has capacity.
+    pub(super) fn insert(&mut self, index: usize) {
+        if self.limit == 0 {
+            return;
+        }
+        let entry = AlphabeticalEntry {
+            index,
+            key: (self.key_for_index)(index),
+        };
+        if self.heap.len() < self.limit {
+            self.heap.push(entry);
+            self.dirty = true;
+        } else if let Some(mut current_max) = self.heap.peek_mut()
+            && entry < *current_max
+        {
+            *current_max = entry;
+            self.dirty = true;
+        }
+    }
+
+    /// Emits an incremental update when new items were inserted.
+    pub(super) fn flush_partial(&mut self) -> bool {
+        if !self.dirty {
+            return true;
+        }
+        self.emit(false)
+    }
+
+    /// Emits the final alphabetical set.
+    pub(super) fn finish(&mut self) -> bool {
+        if self.limit == 0 {
+            return self.emit(true);
+        }
+
+        if !self.emit(true) {
+            return false;
+        }
+        true
+    }
+
+    fn emit(&mut self, complete: bool) -> bool {
+        if self.limit == 0 {
+            return self
+                .tx
+                .send(SearchResult {
+                    id: self.id,
+                    mode: self.mode,
+                    indices: Vec::new(),
+                    scores: Vec::new(),
+                    complete,
+                })
+                .is_ok();
+        }
+
+        self.scratch.clear();
+        self.scratch.extend(self.heap.iter().cloned());
+        self.scratch
+            .sort_unstable_by(|a, b| a.key.cmp(&b.key).then_with(|| a.index.cmp(&b.index)));
+
+        let mut indices = Vec::with_capacity(self.scratch.len());
+        for entry in &self.scratch {
+            indices.push(entry.index);
+        }
+        let scores = vec![0; indices.len()];
+
+        match self.tx.send(SearchResult {
+            id: self.id,
+            mode: self.mode,
+            indices,
+            scores,
+            complete,
+        }) {
+            Ok(()) => {
+                self.sent_any = true;
+                self.dirty = false;
+                true
+            }
+            Err(_) => false,
+        }
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn keeps_smallest_entries() {
+        let (tx, rx) = std::sync::mpsc::channel();
+        let mut collector = AlphabeticalCollector::new(9, SearchMode::Files, &tx, 5, |idx| {
+            ["z", "b", "a", "y", "c"][idx].to_string()
+        });
+
+        for index in 0..5 {
+            collector.insert(index);
+        }
+        assert!(collector.finish());
+
+        let result = rx.try_recv().expect("collector should emit");
+        assert_eq!(result.indices, vec![2, 1, 4, 3, 0]);
+        assert_eq!(result.scores, vec![0, 0, 0, 0, 0]);
+    }
+
+    #[test]
+    fn handles_empty_dataset() {
+        let (tx, rx) = std::sync::mpsc::channel();
+        let mut collector = AlphabeticalCollector::new(3, SearchMode::Facets, &tx, 0, |_| "".into());
+        assert!(collector.finish());
+        let result = rx.try_recv().expect("empty collector should emit");
+        assert!(result.indices.is_empty());
+        assert!(result.scores.is_empty());
+    }
+}
diff --git a/src/search/commands.rs b/src/search/commands.rs
new file mode 100644
index 0000000000000000000000000000000000000000..b9fa46632df8a2878ee15c705bee6820cf8613ac
--- /dev/null
+++ b/src/search/commands.rs
@@ -0,0 +1,39 @@
+use crate::types::SearchMode;
+
+#[cfg(feature = "fs")]
+use crate::indexing::IndexUpdate;
+
+/// Commands understood by the background search worker.
+#[derive(Debug)]
+pub(crate) enum SearchCommand {
+    /// Run a fuzzy search for the provided query and mode.
+    Query {
+        /// Identifier that allows the UI to correlate responses with the originating query.
+        id: u64,
+        /// User supplied query string.
+        query: String,
+        /// Which data set to search.
+        mode: SearchMode,
+    },
+    /// Merge a fresh index update into the existing in-memory search data.
+    #[cfg(feature = "fs")]
+    Update(IndexUpdate),
+    /// Stop the background worker thread.
+    Shutdown,
+}
+
+/// Aggregated search results emitted back to the UI layer.
+#[derive(Debug)]
+pub(crate) struct SearchResult {
+    /// Identifier matching the [`SearchCommand::Query`] that produced the result.
+    pub(crate) id: u64,
+    /// Data set that was searched.
+    pub(crate) mode: SearchMode,
+    /// Offsets into the `SearchData` arrays that matched.
+    pub(crate) indices: Vec<usize>,
+    /// Scores associated with each index.
+    pub(crate) scores: Vec<u16>,
+    #[allow(dead_code)]
+    /// Whether the worker streamed the complete result set.
+    pub(crate) complete: bool,
+}
diff --git a/src/search/config.rs b/src/search/config.rs
new file mode 100644
index 0000000000000000000000000000000000000000..ae7dd71308b6273091613a6cadeede8ba9eb8e77
--- /dev/null
+++ b/src/search/config.rs
@@ -0,0 +1,55 @@
+use frizbee::Options;
+
+use super::PREFILTER_ENABLE_THRESHOLD;
+
+/// Builds fuzzy matching options for the provided query and dataset size.
+pub(crate) fn config_for_query(query: &str, dataset_len: usize) -> Options {
+    let mut config = Options {
+        prefilter: false,
+        ..Options::default()
+    };
+
+    let length = query.chars().count();
+    let mut allowed_typos: u16 = match length {
+        0 => 0,
+        1 => 0,
+        2..=4 => 1,
+        5..=7 => 2,
+        8..=12 => 3,
+        _ => 4,
+    };
+    if let Ok(max_reasonable) = u16::try_from(length.saturating_sub(1)) {
+        allowed_typos = allowed_typos.min(max_reasonable);
+    }
+
+    if dataset_len >= PREFILTER_ENABLE_THRESHOLD {
+        config.prefilter = true;
+        config.max_typos = Some(allowed_typos);
+    } else {
+        config.prefilter = false;
+        config.max_typos = None;
+    }
+
+    config.sort = false;
+
+    config
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+
+    #[test]
+    fn enables_prefilter_for_large_datasets() {
+        let config = config_for_query("example", PREFILTER_ENABLE_THRESHOLD);
+        assert!(config.prefilter);
+        assert_eq!(config.max_typos, Some(2));
+    }
+
+    #[test]
+    fn disables_prefilter_for_small_datasets() {
+        let config = config_for_query("example", PREFILTER_ENABLE_THRESHOLD - 1);
+        assert!(!config.prefilter);
+        assert_eq!(config.max_typos, None);
+    }
+}
diff --git a/src/search/mod.rs b/src/search/mod.rs
new file mode 100644
index 0000000000000000000000000000000000000000..a817ae90f68409448a5583f2f3317c25623859d0
--- /dev/null
+++ b/src/search/mod.rs
@@ -0,0 +1,15 @@
+mod alphabetical;
+mod aggregator;
+mod commands;
+mod config;
+mod streaming;
+mod worker;
+
+pub(crate) use commands::{SearchCommand, SearchResult};
+pub(crate) use config::config_for_query;
+pub(crate) use worker::spawn;
+
+pub(crate) const PREFILTER_ENABLE_THRESHOLD: usize = 1_000;
+pub(crate) const MAX_RENDERED_RESULTS: usize = 2_000;
+const MATCH_CHUNK_SIZE: usize = 512;
+const EMPTY_QUERY_BATCH: usize = 128;
diff --git a/src/search/streaming.rs b/src/search/streaming.rs
new file mode 100644
index 0000000000000000000000000000000000000000..8ba30810fec5e6136439885e3acd8ae72df30530
--- /dev/null
+++ b/src/search/streaming.rs
@@ -0,0 +1,199 @@
+use std::sync::atomic::{AtomicU64, Ordering as AtomicOrdering};
+use std::sync::mpsc::Sender;
+
+use frizbee::match_list;
+
+use super::aggregator::ScoreAggregator;
+use super::alphabetical::AlphabeticalCollector;
+use super::commands::SearchResult;
+use super::{config::config_for_query, EMPTY_QUERY_BATCH, MATCH_CHUNK_SIZE};
+use crate::types::{SearchData, SearchMode};
+
+/// Streams facet matches for the given query back to the UI thread.
+pub(super) fn stream_facets(
+    data: &SearchData,
+    query: &str,
+    id: u64,
+    tx: &Sender<SearchResult>,
+    latest_query_id: &AtomicU64,
+) -> bool {
+    let trimmed = query.trim();
+    if trimmed.is_empty() {
+        return stream_alphabetical_facets(data, id, tx, latest_query_id);
+    }
+
+    let config = config_for_query(trimmed, data.facets.len());
+    let mut aggregator = ScoreAggregator::new(id, SearchMode::Facets, tx);
+    let mut offset = 0;
+    for chunk in data.facets.chunks(MATCH_CHUNK_SIZE) {
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        let haystacks: Vec<&str> = chunk.iter().map(|facet| facet.name.as_str()).collect();
+        let matches = match_list(trimmed, &haystacks, config);
+        for entry in matches {
+            if entry.score == 0 {
+                continue;
+            }
+            let index = offset + entry.index_in_haystack as usize;
+            aggregator.push(index, entry.score);
+        }
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        if !aggregator.flush_partial() {
+            return false;
+        }
+        offset += chunk.len();
+    }
+
+    if should_abort(id, latest_query_id) {
+        return true;
+    }
+
+    aggregator.finish()
+}
+
+/// Streams file matches for the given query back to the UI thread.
+pub(super) fn stream_files(
+    data: &SearchData,
+    query: &str,
+    id: u64,
+    tx: &Sender<SearchResult>,
+    latest_query_id: &AtomicU64,
+) -> bool {
+    let trimmed = query.trim();
+    if trimmed.is_empty() {
+        return stream_alphabetical_files(data, id, tx, latest_query_id);
+    }
+
+    let config = config_for_query(trimmed, data.files.len());
+    let mut aggregator = ScoreAggregator::new(id, SearchMode::Files, tx);
+    let mut offset = 0;
+    for chunk in data.files.chunks(MATCH_CHUNK_SIZE) {
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        let haystacks: Vec<&str> = chunk.iter().map(|file| file.search_text()).collect();
+        let matches = match_list(trimmed, &haystacks, config);
+        for entry in matches {
+            if entry.score == 0 {
+                continue;
+            }
+            let index = offset + entry.index_in_haystack as usize;
+            aggregator.push(index, entry.score);
+        }
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        if !aggregator.flush_partial() {
+            return false;
+        }
+        offset += chunk.len();
+    }
+
+    if should_abort(id, latest_query_id) {
+        return true;
+    }
+
+    aggregator.finish()
+}
+
+fn stream_alphabetical_facets(
+    data: &SearchData,
+    id: u64,
+    tx: &Sender<SearchResult>,
+    latest_query_id: &AtomicU64,
+) -> bool {
+    let mut collector =
+        AlphabeticalCollector::new(id, SearchMode::Facets, tx, data.facets.len(), |index| {
+            data.facets[index].name.clone()
+        });
+
+    let mut processed = 0;
+    for (index, _) in data.facets.iter().enumerate() {
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        collector.insert(index);
+        processed += 1;
+        if processed % EMPTY_QUERY_BATCH == 0 {
+            if should_abort(id, latest_query_id) {
+                return true;
+            }
+            if !collector.flush_partial() {
+                return false;
+            }
+        }
+    }
+
+    if should_abort(id, latest_query_id) {
+        return true;
+    }
+
+    collector.finish()
+}
+
+fn stream_alphabetical_files(
+    data: &SearchData,
+    id: u64,
+    tx: &Sender<SearchResult>,
+    latest_query_id: &AtomicU64,
+) -> bool {
+    let mut collector =
+        AlphabeticalCollector::new(id, SearchMode::Files, tx, data.files.len(), |index| {
+            data.files[index].path.clone()
+        });
+
+    let mut processed = 0;
+    for (index, _) in data.files.iter().enumerate() {
+        if should_abort(id, latest_query_id) {
+            return true;
+        }
+        collector.insert(index);
+        processed += 1;
+        if processed % EMPTY_QUERY_BATCH == 0 {
+            if should_abort(id, latest_query_id) {
+                return true;
+            }
+            if !collector.flush_partial() {
+                return false;
+            }
+        }
+    }
+
+    if should_abort(id, latest_query_id) {
+        return true;
+    }
+
+    collector.finish()
+}
+
+fn should_abort(id: u64, latest_query_id: &AtomicU64) -> bool {
+    latest_query_id.load(AtomicOrdering::Acquire) != id
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::types::{FacetRow, FileRow};
+
+    fn sample_data() -> SearchData {
+        SearchData::default()
+            .with_facets(vec![FacetRow::new("alpha", 1), FacetRow::new("beta", 1)])
+            .with_files(vec![
+                FileRow::new("src/lib.rs", Vec::<String>::new()),
+                FileRow::new("src/main.rs", Vec::<String>::new()),
+            ])
+    }
+
+    #[test]
+    fn aborts_when_query_id_changes() {
+        let data = sample_data();
+        let (tx, rx) = std::sync::mpsc::channel();
+        let latest = AtomicU64::new(42);
+
+        assert!(stream_files(&data, "alpha", 41, &tx, &latest));
+        assert!(rx.try_recv().is_err());
+    }
+}
diff --git a/src/search/worker.rs b/src/search/worker.rs
new file mode 100644
index 0000000000000000000000000000000000000000..1433be2c6570b81c027469cfdbf262e827909fbf
--- /dev/null
+++ b/src/search/worker.rs
@@ -0,0 +1,81 @@
+use std::sync::atomic::AtomicU64;
+use std::sync::mpsc::{self, Receiver, Sender};
+use std::sync::Arc;
+use std::thread;
+
+use super::commands::{SearchCommand, SearchResult};
+use super::streaming::{stream_facets, stream_files};
+use crate::types::{SearchData, SearchMode};
+
+#[cfg(feature = "fs")]
+use crate::indexing::merge_update;
+
+/// Launches the background search worker thread and returns communication channels.
+#[cfg_attr(not(feature = "fs"), allow(unused_mut))]
+pub(crate) fn spawn(
+    mut data: SearchData,
+) -> (
+    Sender<SearchCommand>,
+    Receiver<SearchResult>,
+    Arc<AtomicU64>,
+) {
+    let (command_tx, command_rx) = mpsc::channel();
+    let (result_tx, result_rx) = mpsc::channel();
+    let latest_query_id = Arc::new(AtomicU64::new(0));
+    let thread_latest = Arc::clone(&latest_query_id);
+
+    thread::spawn(move || worker_loop(&mut data, command_rx, result_tx, thread_latest));
+
+    (command_tx, result_rx, latest_query_id)
+}
+
+fn worker_loop(
+    data: &mut SearchData,
+    command_rx: Receiver<SearchCommand>,
+    result_tx: Sender<SearchResult>,
+    latest_query_id: Arc<AtomicU64>,
+) {
+    while let Ok(command) = command_rx.recv() {
+        if !handle_command(data, &result_tx, &latest_query_id, command) {
+            break;
+        }
+    }
+}
+
+fn handle_command(
+    data: &mut SearchData,
+    result_tx: &Sender<SearchResult>,
+    latest_query_id: &Arc<AtomicU64>,
+    command: SearchCommand,
+) -> bool {
+    match command {
+        SearchCommand::Query { id, query, mode } => match mode {
+            SearchMode::Facets => {
+                stream_facets(data, &query, id, result_tx, latest_query_id.as_ref())
+            }
+            SearchMode::Files => {
+                stream_files(data, &query, id, result_tx, latest_query_id.as_ref())
+            }
+        },
+        #[cfg(feature = "fs")]
+        SearchCommand::Update(update) => {
+            merge_update(data, &update);
+            true
+        }
+        SearchCommand::Shutdown => false,
+    }
+}
+
+#[cfg(test)]
+mod tests {
+    use super::*;
+    use crate::types::SearchData;
+
+    #[test]
+    fn shutdown_command_stops_worker() {
+        let data = SearchData::default();
+        let (tx, _rx, latest) = spawn(data);
+        assert_eq!(latest.load(std::sync::atomic::Ordering::Relaxed), 0);
+        tx.send(SearchCommand::Shutdown).unwrap();
+    }
+}
